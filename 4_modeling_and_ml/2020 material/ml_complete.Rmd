---
title: "Session 3: Modeling and Machine Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE}
library(modelr)
library(quantreg)
library(glmnet)
library(tidyverse)
```


## The Data Science Process

![The Data Science Process](data-science-process.eps)

Now that you've had some experience using R for Steps 2, 3, and 6, we're going to focus on Steps 4 and 5 in this session.

Model Selection (Step 4) means choosing an algorithm which acts on the data available to us, and produces a solution to the task defined in Step 1.

Model Evaluation (Step 5) means assessing how well the solution obtained in Step 4 solves the task in Step 1.

We're going to get some practice following these steps for three common classes of task in machine learning:

1. Regression (supervised learning)
2. Classification (supervised learning)
3. Clustering (unsupervised learning)

It's important to note that these steps are never followed linearly in practice, but instead used for iteration. We'll try to get a sense of what this looks like in this session.


## Regression

We're going to start by solving a regression problem. But first, what is a regression problem?

**Inputs**

Regression is a supervised learning problem, which means that we have access to a dataset $\mathcal{D} = \{ (x_i, y_i) \}_{i = 1}^{n}$ where each $x_i \in \mathbb{R}^d$ and each $y_i \in \mathbb{R}$.

Note that for regression, the output is *continuous*.

**Aim**

The aim of a regression task is to find a function $h: \mathbb{R}^d \to \mathbb{R}$ which allows us to find an accurate prediction of the output, $y$, for any given input, $x$. Exactly what we mean by *accuracy* will be discussed when we talk about model evaluation.

The way we approach the problem of finding $h$ is by selecting a parameterised class of models, and solving an optimisation problem to find the best model within this class. We usually repeat this process for several model classes.


### Defining our Regression Task

We're going to use the Boston Airbnb dataset, where each entry is a property listing, and aim to use the input variables available to us in order to predict the price of a listing.

Note here that we haven't specified exactly which variables will be used to define each $x_i$. This is a choice which is part of the modeling process.


### Data Wrangling (TODO: decide whether to keep this in, or just supply a clean dataset)

```{r}
# Load.
listingsOrig <- read.csv("../../data/listings.csv", stringsAsFactors = FALSE)

# Process.
listings <- listingsOrig %>%
  mutate(price = parse_number(price)) %>%  # Get numbers from price strings.
  filter(accommodates <= 10, price <= 1000) %>%  # Remove outliers.
  filter(property_type %in%
           c("Apartment", "House", "Bed & Breakfast", "Condominium", "Loft", "Townhouse"),	
         !(neighbourhood_cleansed %in%
             c("Leather District", "Longwood Medical Area"))) %>%  # Get house types and neighbourhoods.
  select_if(~sum(is.na(.)) < 0.9 * nrow(listingsOrig)) %>%  # Choose only mainly complete columns.
  mutate_if(is.integer, as.numeric) %>%  # Convert integer columns to numeric.
  mutate_all(~coalesce(., median(., na.rm = TRUE)))  # Replace NA values with column medians.
```


### Ordinary Least Squares

The first class of models we're going to try is linear models. This means we hypothesis that the output, $y$, can be described using a linear combination of the inputs, $x$: $h(x) = w^{\intercal} x$.

Ordinary Least Squares (OLS) finds the best fitting linear model according to the least squares loss function:

$\min_w \: \frac{1}{n} ||X w - Y||_2^2$

where $e_{i}^{\intercal} X = x_{i}^{\intercal}$, and $e_{i}^{\intercal} Y = y_i$. This is equivalent to:

$\min_w \: \frac{1}{n} \sum_{i = 1}^{n} (w^{\intercal} x_i - y_i)^2$

We won't talk about the details behind solving this problem, but it can be written in closed form.

**Exercise**

What are the tunable parameters for the class of OLS models?

...

Now, we need to choose exactly which inputs will be used for fitting the model. Which inputs might be predictive of price? Let's have a look at all the variable names:

```{r}
sort(names(listings))
```

For a very simple model, let's just choose `accommodates` as the only input. To fit this model we use the base R function `lm`:

```{r}
ols_model <- lm(price ~ accommodates, data = listings)
```

What's happening here? The first argument makes use of R's formula interface. In words, we want the output `price` to be explained by the input `accommodates`.

The second (named) argument supplies the data we're using to build the model. This is where R looks for the names (`price` and `accommodates`) contained in the formula.

Let's check out the `ols_model` object. It's a list of a bunch of relevant information generated by the function call, and we can use `$` to view different elements:

```{r}
names(ols_model)
```

The function `summary` is overloaded for many different objects and often gives a useful snapshot of the model:

```{r}
summary(ols_model)
```

First, let's look at the 'coefficients' section. Notice that R automatically adds an intercept term. In the 'estimate' column, we see that the point estimates for the linear model here say that the price is \$55.20 plus \$37.79 for every person accommodated. Notice the '***' symbols at the end of the '(Intercept)' and 'accommodates' rows indicate that according to a statistical t-test, both coefficients are significantly different from 0.

As another check on the quality of the model, let's plot the fitted line. There are some nifty functions in the `modelr` package that make interacting with models easy within the `tidyverse` setting. We'll use `modelr::add_predictions()` here:

```{r}
listings %>%	
  add_predictions(ols_model, var = "pred") %>%
  ggplot(aes(x = accommodates)) +
  geom_point(aes(y = price)) +
  geom_line(aes(y = pred), color = 'red') +
  labs(title = "OLS Model Fit")
```

Nice. We can also remove the linear trend and check the residual uncertainty, which we'll do here using `modelr::add_residuals()`.

This is helpful to check whether the residuals looks like random noise rather than an unidentified trend (which would indicate that a linear model is not a good hypothesis for the relationship).

```{r}
listings %>%
  add_residuals(ols_model, var = "resid") %>%
  ggplot(aes(x = accommodates, y = resid)) + 
  geom_point() +
  labs(title = "OLS Model Residuals")
```

Due to the closely spaced values, box plots might tell a better story here:

```{r}
listings %>%	
  add_residuals(ols_model, var = "resid") %>%	
  group_by(as.factor(accommodates)) %>%	
  ggplot(aes(x = as.factor(accommodates), y = resid)) + 
  geom_boxplot() +
  labs(title = "OLS Model Residual Boxplots")
```

The residuals seem relatively centered around zero, though there does appear to be some right skew. 9 and 10-person accommodation is also less centered. Perhaps the model doesn't apply so well here -- why might that be?


### Evaluation and Iteration

#### Training, Validation and Testing Splits

Recall that our ultimate goal in this supervised learning task is to be able to predict price ($y$) from an unseen set of inputs ($x$, which we are still playing around with).

When we were learning the OLS formulation, and how to fit the model, we used the entire dataset. Simply taking the mean squared error on this dataset as a measure of performance is clearly not fair -- we want the model to generalise to unseen data.

Put another way: an obvious best solution on the entire dataset is to use a nearest neighbours classifier...

To address this problem, we split the data into three different chunks:

- Training data: the data we build our models on.
- Validation data: the data we tune hyperparameters on.
- Testing data: the data we obtain a final estimate of performance on.

Note that the validation set is like an 'intermediate' estimate of performance, which we use to tune parameters when building models within a specific model class. If we didn't use the validation set for this purpose, the only ways of selecting the best model from a class would be to look at it's performance on the training set (not good, for previous reasons) or the testing set (also not good, for previous reasons).

So, given these data splits, the process we follow *within a model class* is:

1. Build a collection of models on training set.
2. Evaluate performance of models on validation set.
3. Choose best model from within class, and obtain estimate of its performance by using the testing set.
4. Choose best model across all model classes according to testing set.

```{r}
part <- resample_partition(listings, c(train = .6, val = .2, test = .2))
```

```{r}
part$train 
```

```{r}
part$train %>% as_tibble()
```

There are many different ways we can evaluate the performance of a model on the testing set. The two we will use are the mean squared error, and the R^2 coefficient.

Describe, example calculation...

### Iteration

We have glossed over the precise choice of variables to use, so let's try a few different combinations. This sounds like a job for a loop, but since we're in the dplyr framework it's better to think of writing a function where the loop variables are the input.

We'll write a function that accepts a training/validation/testing partition object, and formula. Note that the lm function also works with a string supplied for the formula:

```{r}
lm("price ~ accommodates", data = listings)
```

```{r}
calc_mse <- function(model, data) {
  
  resids <- data %>%
    add_residuals(model) %>%
    pull(resid)
  
  return(mean(resids ^ 2))
}
```

Now a function to take an individual formula and dataset split, and return performance metrics (including the stored model itself).

```{r}
eval_ols <- function(form, part) {
  ols <- lm(form, data = part$train)
  
  result <- tibble(model = list(ols),
                   formula = form,
                   train_mse = calc_mse(ols, part$train %>% as_tibble()),
                   val_mse = calc_mse(ols, part$val %>% as_tibble()),
                   test_mse = calc_mse(ols, part$test %>% as_tibble()))
  
  return(result)
}
```

Now that we have the functions in place, let's create list of formulae to try.

```{r}
forms <- list(
  "price ~ accommodates",
  "price ~ accommodates + review_scores_rating"
)
```

And then use the tidyverse to build a whole series of models together.

```{r}
ols_models <- forms %>%
  map(~eval_ols(., part)) %>%
  bind_rows()
ols_models
```

Adding amenities...


### Median Regression

Since we're dealing with data on price, we expect that there will be high outliers. While least-squares regression is reliable in many settings, it has the property  that the estimates it generates depend quite a bit on the outliers.

One alternative, median regression, minimizes *absolute* error rather than squared error:

$ \min_w \sum_{i = 1}^{n} |x_i^\intercal w - y_i|$

where $X = [x_i^\intercal \quad 1]$ and $Y = [y_i]$.

This has the effect of regressing on the median rather than the mean, and is more robust to outliers. In R, the `quantreg` package implements this approach.

For this exercise, install the quantreg package, and compare the behavior of the median regression fit (using the `rq()`) function) to the least squares fit from `lm()` on the original listings data set given below which includes all the price outliers.

Modify the function we wrote about to 

```{r}
eval_med <- function(form, part) {
  ols <- lm(form, data = part$train)
  
  result <- tibble(model = list(ols),
                   formula = form,
                   train_mse = calc_mse(ols, part$train %>% as_tibble()),
                   val_mse = calc_mse(ols, part$val %>% as_tibble()),
                   test_mse = calc_mse(ols, part$test %>% as_tibble()))
  
  return(result)
}
```

```{r}
ols <- lm(price ~ accommodates, data = part$train %>% as_tibble())
med <- rq(price ~ accommodates, data = part$train %>% as_tibble())
summary(ols)
summary(med)

part$test %>%
  as_tibble() %>%
  gather_predictions(ols, med) %>%
  ggplot(aes(x = accommodates)) +	
  geom_point(aes(y = price)) +	
  geom_line(aes(y = pred, color = model))
```

Need to use heaps of variables, obtain overfitting and lead into regularisation...


### Regularisation

Regularisation helps to avoid overfitting by penalising model complexity. Mathematically, we add a term to the optimisation problem that is solved when fitting a model. Recall that the OLS we've worked with is:

$\min_w \sum_{i = 1}^{n} (x_i^\intercal w - y_i)^2$

With a regularisation term, this becomes:

$\min_w \sum_{i = 1}^{n} (x_i^\intercal w - y_i)^2 + \lambda \Omega(w)$

$\Omega(w)$ is a penality on the complexity of the model. Two common choices for $\Omega(w)$ are:

1. $\Omega(w) = ||w||_2^2$: this is ridge regression.
2. $\Omega(w) = ||w||_1$: this is LASSO.

There are two types of flexibility within this framework:

- Choice of norm, a structural decision.
- Choice of $\lambda$, a parametric decision.	

Both types of regression shrink all the elements of the unconstrained $w$ vector towards zero, some more than others in a special way. LASSO shrinks the coefficients so that some are equal to zero. This feature is nice because it helps us interpret the model by getting rid of the effects of many of the variables.	

For LASSO, we'll use the `glmnet` package. This package illustrates some of the less desirable aspects of the R universe, since it doesn't work very elegantly with the `tidyverse` (it uses matrix representations of the data rather than data frame representations).

Let's check out the function `glmnet()`. We can see the documentation from the command line using `?glmnet`.	
```{r}
library(glmnet)
?glmnet
```

Notice that `glmnet()` doesn't communicate with the data via formulas. Instead, it wants a matrix of predictor variables and a vector of values for the variable we're trying to predict, including all the categorical variables that R automatically expanded into indicator variables. Fortunately, R has a `model.matrix()` function which takes a data frame and gets it into the right form for `glmnet()` and other functions with this type of input.	

First, we need to convert the data to a matrix for X and a vector for Y. We'll use the big_formula that we made previously for our linear regression, but with the dependent variable removed.

```{r}
model_formula <- as.formula(gsub("price", "", paste(big_formula)))
x <- model.matrix(model_formula, data = listingsBig)
y <- as.vector(listingsBig$price)
```

Next, split into training/testing sets

```{r}
set.seed(123)
spl <- sample.split(y, SplitRatio = 0.7)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
```

Finally, let's fit a our first LASSO model. There's a way to specify lambda manually, but let's just accept the default for now and see what we get.

```{r}
lasso_price <- glmnet(xTrain, yTrain)
```

This time the `summary()` function isn't quite as useful:

```{r}
summary(lasso_price)
```

It does give us some info, though. Notice that "lambda" is a vector of length 86. The `glmnet()` function has defined 86 different values of lambda and found the corresponding optimal beta vector for each one! We have 86 different models here.

Let's look at some of the coefficients for the different models. We'll start with one where lambda is really high:
```{r}
lasso_price$lambda[1]
nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?	
```

Here the penalty on the size of the coefficients is so high that R sets them all to zero. Moving to some smaller lambdas:

```{r}
lasso_price$lambda[10]	
lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]	

lasso_price$lambda[20]	
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
```

And, to see the whole path of lambdas:

```{r}
plot(lasso_price, xvar = "lambda")
```

Here, each line is one variable. The plot is quite messy with so many variables, but it gives us the idea. As lambda shrinks, the model adds more and more nonzero coefficients.	


## Cross-Validation

How do we choose which of the 86 models to use? Or in other words, how do we "tune" the $\lambda$ parameter? We'll use a similar idea to the training-test set split called cross-validation.	

The idea behind cross-validation is this: what if we trained our family of models (in this case 86) on only some of the training data and left out some other data points? Then we could use those other data points to figure out which of the lambdas works best out-of-sample. So we'd have a training set for training all the models, a validation set for choosing the best one, and a test set to evaluate performance once we've settled on a model.	

There's just one other trick: since taking more samples reduces noise, could we somehow take more validation set samples? Here's where *cross*-validation comes in.

We divide the training data into groups called *folds*, and for each fold repeat the train-validate procedure on the remaining training data and use the current fold as a validation set. We then average the performance of each model on each fold and pick the best one.	

This is a very common *resampling* method that applies in lots and lots of settings. Lucky for us the glmnet package has a very handy function called `cv.glmnet()` which does the entire process automatically. Let's look at the function arguments using `?cv.glmnet`.	

The relevant arguments for us right now are	
* x, the matrix of predictors	
* y, the response variable	
* nfolds, the number of ways to split the training set (defaults to 10)
* type.measure, the metric of prediction quality. It defaults to mean squared error, the square of RMSE, for linear regression	

Let's do the cross-validation:

```{r}
lasso_price_cv <- cv.glmnet(xTrain, yTrain)
summary(lasso_price_cv) # What does the model object look like?	
```

Notice the "lambda.min". This is the best lambda as determined by the cross validation. "lambda.1se" is the largest lambda such that the "error is within 1 standard error of the minimum."	

There's another automatic plotting function for `cv.glmnet()` which shows the error for each model:	

```{r}
plot(lasso_price_cv)
```

The first vertical dotted line shows `lambda.min`, and the second is `lambda.1se`. The figure illustrates that we cross-validate to find the "sweet spot" where there's not too much bias (high lambda) and not too much noise (low lambda). The left-hand side of this graph is flatter than we'd sometimes see, meaning that the unpenalized model may not be too bad. However, increasing lambda increases interpretability at close to no loss in prediction accuracy!	

Let's again compare training and test error. Because we are using glmnet we need to use the specialized `predict.cv.glmnet()` function:

```{r}
?predict.cv.glmnet

#pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain, s = "lambda.min")
pred_train <- predict(lasso_price_cv, newx = xTrain, s = "lambda.min")
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
  sum((mean(yTrain) - yTrain) ^ 2)
#pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest, s = "lambda.min")
pred_test <- predict(lasso_price_cv, newx = xTest, s = "lambda.min")
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
  sum((mean(yTrain) - yTest) ^ 2)
```

Let's add these as a row to our results data.frame

```{r}
results <- results %>%
  rbind(list(model = "lasso", R2 = R2_lasso, OSR2 = OSR2_lasso))
results
```

The overfitting problem has gotten better, but hasn't yet gone away completely. I added a bunch variables for dramatic effect that we could probably screen out before running the LASSO if we really wanted a good model.

One more note on cross-validation: the `glmnet` package has built-in functionality for cross-validation. In situations where that's not the case, `modelr::crossv_kfold()` will prepare data for cross-validation in a nice way.