---
title: "Session 4: Modeling and Machine Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE}
library(modelr)
library(quantreg)
library(glmnet)
library(tidyverse)
library(caTools)
library(randomForest)
library(ROCR)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(leaflet)
```


## The Data Science Process

![The Data Science Process](data-science-process.eps)

Now that you've had some experience using R for Steps 2, 3, and 6, we're going to focus on Steps 4 and 5 in this session.

Model Selection (Step 4) means choosing an algorithm which acts on the data available to us, and produces a solution to the task defined in Step 1.

Model Evaluation (Step 5) means assessing how well the solution obtained in Step 4 solves the task in Step 1.

We're going to get some practice following these steps for three common classes of task in machine learning:

1. Regression (supervised learning)
2. Classification (supervised learning)
3. Clustering (unsupervised learning)

It's important to note that these steps are never followed linearly in practice, but instead used for iteration. We'll try to get a sense of what this looks like in this session.


## Regression

We're going to start by solving a regression problem. But first, what is a regression problem?

**Inputs**

Regression is a supervised learning problem, which means that we have access to a dataset $\mathcal{D} = \{ (x_i, y_i) \}_{i = 1}^{n}$ where each $x_i \in \mathbb{R}^d$ and each $y_i \in \mathbb{R}$.

Note that for regression, the output is *continuous*.

**Aim**

The aim of a regression task is to find a function $h: \mathbb{R}^d \to \mathbb{R}$ which allows us to find an accurate prediction of the output, $y$, for any given input, $x$. Exactly what we mean by *accuracy* will be discussed when we talk about model evaluation.

The way we approach the problem of finding $h$ is by selecting a parameterised class of models, and solving an optimisation problem to find the best model within this class. We usually repeat this process for several model classes.


### Defining our Regression Task

We're going to use the Boston Airbnb dataset, where each entry is a property listing, and aim to use the input variables available to us in order to predict the price of a listing.

Note here that we haven't specified exactly which variables will be used to define each $x_i$. This is a choice which is part of the modeling process.


### Data Wrangling (TODO: decide whether to keep this in, or just supply a clean dataset)

```{r}
# Load.
listingsOrig <- read.csv("../../data/listings.csv", stringsAsFactors = FALSE)

# Process.
listings <- listingsOrig %>%
  mutate(price = parse_number(price)) %>%  # Get numbers from price strings.
  filter(accommodates <= 10, price <= 1000) %>%  # Remove outliers.
  filter(property_type %in%
           c("Apartment", "House", "Bed & Breakfast", "Condominium", "Loft", "Townhouse"),	
         !(neighbourhood_cleansed %in%
             c("Leather District", "Longwood Medical Area"))) %>%  # Get house types and neighbourhoods.
  select_if(~sum(is.na(.)) < 0.9 * nrow(listingsOrig)) %>%  # Choose only mainly complete columns.
  mutate_if(is.integer, as.numeric) %>%  # Convert integer columns to numeric.
  mutate_all(~coalesce(., median(., na.rm = TRUE)))  # Replace NA values with column medians.
```


### Ordinary Least Squares

The first class of models we're going to try is linear models. This means we hypothesis that the output, $y$, can be described using a linear combination of the inputs, $x$: $h(x) = w^{\intercal} x$.

Ordinary Least Squares (OLS) finds the best fitting linear model according to the least squares loss function:

$\min_w \: \frac{1}{n} ||X w - Y||_2^2$

where $e_{i}^{\intercal} X = x_{i}^{\intercal}$, and $e_{i}^{\intercal} Y = y_i$. This is equivalent to:

$\min_w \: \frac{1}{n} \sum_{i = 1}^{n} (w^{\intercal} x_i - y_i)^2$

We won't talk about the details behind solving this problem, but it can be written in closed form.

**Exercise**

What are the tunable parameters for the class of OLS models?

...

Now, we need to choose exactly which inputs will be used for fitting the model. Which inputs might be predictive of price? Let's have a look at all the variable names:

```{r}
sort(names(listings))
```

For a very simple model, let's just choose `accommodates` as the only input. To fit this model we use the base R function `lm`:

```{r}
ols_model <- lm(price ~ accommodates, data = listings)
```

What's happening here? The first argument makes use of R's formula interface. In words, we want the output `price` to be explained by the input `accommodates`.

The second (named) argument supplies the data we're using to build the model. This is where R looks for the names (`price` and `accommodates`) contained in the formula.

Let's check out the `ols_model` object. It's a list of a bunch of relevant information generated by the function call, and we can use `$` to view different elements:

```{r}
names(ols_model)
```

The function `summary` is overloaded for many different objects and often gives a useful snapshot of the model:

```{r}
summary(ols_model)
```

First, let's look at the 'coefficients' section. Notice that R automatically adds an intercept term. In the 'estimate' column, we see that the point estimates for the linear model here say that the price is \$55.20 plus \$37.79 for every person accommodated. Notice the '***' symbols at the end of the '(Intercept)' and 'accommodates' rows indicate that according to a statistical t-test, both coefficients are significantly different from 0.

As another check on the quality of the model, let's plot the fitted line. There are some nifty functions in the `modelr` package that make interacting with models easy within the `tidyverse` setting. We'll use `modelr::add_predictions()` here:

```{r}
listings %>%	
  add_predictions(ols_model, var = "pred") %>%
  ggplot(aes(x = accommodates)) +
  geom_point(aes(y = price)) +
  geom_line(aes(y = pred), color = 'red') +
  labs(title = "OLS Model Fit")
```

Nice. We can also remove the linear trend and check the residual uncertainty, which we'll do here using `modelr::add_residuals()`.

This is helpful to check whether the residuals looks like random noise rather than an unidentified trend (which would indicate that a linear model is not a good hypothesis for the relationship).

```{r}
listings %>%
  add_residuals(ols_model, var = "resid") %>%
  ggplot(aes(x = accommodates, y = resid)) + 
  geom_point() +
  labs(title = "OLS Model Residuals")
```

Due to the closely spaced values, box plots might tell a better story here:

```{r}
listings %>%	
  add_residuals(ols_model, var = "resid") %>%	
  group_by(as.factor(accommodates)) %>%	
  ggplot(aes(x = as.factor(accommodates), y = resid)) + 
  geom_boxplot() +
  labs(title = "OLS Model Residual Boxplots")
```

The residuals seem relatively centered around zero, though there does appear to be some right skew. 9 and 10-person accommodation is also less centered. Perhaps the model doesn't apply so well here -- why might that be?


### Evaluation and Iteration

#### Training, Validation and Testing Splits

Recall that our ultimate goal in this supervised learning task is to be able to predict price ($y$) from an unseen set of inputs ($x$, which we are still playing around with).

When we were learning the OLS formulation, and how to fit the model, we used the entire dataset. Simply taking the mean squared error on this dataset as a measure of performance is clearly not fair -- we want the model to generalise to unseen data.

Put another way: an obvious best solution on the entire dataset is to use a nearest neighbours classifier...

To address this problem, we split the data into three different chunks:

- Training data: the data we build our models on.
- Validation data: the data we tune hyperparameters on.
- Testing data: the data we obtain a final estimate of performance on.

Note that the validation set is like an 'intermediate' estimate of performance, which we use to tune parameters when building models within a specific model class. If we didn't use the validation set for this purpose, the only ways of selecting the best model from a class would be to look at it's performance on the training set (not good, for previous reasons) or the testing set (also not good, for previous reasons).

So, given these data splits, the process we follow *within a model class* is:

1. Build a collection of models on training set.
2. Evaluate performance of models on validation set.
3. Choose best model from within class, and obtain estimate of its performance by using the testing set.
4. Choose best model across all model classes according to testing set.

```{r}
part <- resample_partition(listings, c(train = .6, val = .2, test = .2))
```

```{r}
part$train 
```

```{r}
part$train %>% as_tibble()
```

There are many different ways we can evaluate the performance of a model on the testing set. The two we will use are the mean squared error, and the R^2 coefficient.

Describe, example calculation...

### Iteration

We have glossed over the precise choice of variables to use, so let's try a few different combinations. This sounds like a job for a loop, but since we're in the dplyr framework it's better to think of writing a function where the loop variables are the input.

We'll write a function that accepts a training/validation/testing partition object, and formula. Note that the lm function also works with a string supplied for the formula:

```{r}
lm("price ~ accommodates", data = listings)
```

```{r}
calc_mse <- function(model, data) {
  
  resids <- data %>%
    add_residuals(model) %>%
    pull(resid)
  
  return(mean(resids ^ 2))
}
```

Now a function to take an individual formula and dataset split, and return performance metrics (including the stored model itself).

```{r}
eval_ols <- function(form, part) {
  ols <- lm(form, data = part$train)
  
  result <- tibble(model = list(ols),
                   formula = form,
                   train_mse = calc_mse(ols, part$train %>% as_tibble()),
                   val_mse = calc_mse(ols, part$val %>% as_tibble()),
                   test_mse = calc_mse(ols, part$test %>% as_tibble()))
  
  return(result)
}
```

Now that we have the functions in place, let's create list of formulae to try.

```{r}
forms <- list(
  "price ~ accommodates",
  "price ~ accommodates + review_scores_rating"
)
```

And then use the tidyverse to build a whole series of models together.

```{r}
ols_models <- forms %>%
  map(~eval_ols(., part)) %>%
  bind_rows()
ols_models
```

Adding amenities...


### Median Regression

Since we're dealing with data on price, we expect that there will be high outliers. While least-squares regression is reliable in many settings, it has the property  that the estimates it generates depend quite a bit on the outliers.

One alternative, median regression, minimizes *absolute* error rather than squared error:

$ \min_w \sum_{i = 1}^{n} |x_i^\intercal w - y_i|$

where $X = [x_i^\intercal \quad 1]$ and $Y = [y_i]$.

This has the effect of regressing on the median rather than the mean, and is more robust to outliers. In R, the `quantreg` package implements this approach.

For this exercise, install the quantreg package, and compare the behavior of the median regression fit (using the `rq()`) function) to the least squares fit from `lm()` on the original listings data set given below which includes all the price outliers.

Modify the function we wrote about to 

```{r}
eval_med <- function(form, part) {
  ols <- lm(form, data = part$train)
  
  result <- tibble(model = list(ols),
                   formula = form,
                   train_mse = calc_mse(ols, part$train %>% as_tibble()),
                   val_mse = calc_mse(ols, part$val %>% as_tibble()),
                   test_mse = calc_mse(ols, part$test %>% as_tibble()))
  
  return(result)
}
```

```{r}
ols <- lm(price ~ accommodates, data = part$train %>% as_tibble())
med <- rq(price ~ accommodates, data = part$train %>% as_tibble())
summary(ols)
summary(med)

part$test %>%
  as_tibble() %>%
  gather_predictions(ols, med) %>%
  ggplot(aes(x = accommodates)) +	
  geom_point(aes(y = price)) +	
  geom_line(aes(y = pred, color = model))
```

Need to use heaps of variables, obtain overfitting and lead into regularisation...


<!-- ### Regularisation -->

<!-- Regularisation helps to avoid overfitting by penalising model complexity. Mathematically, we add a term to the optimisation problem that is solved when fitting a model. Recall that the OLS we've worked with is: -->

<!-- $\min_w \sum_{i = 1}^{n} (x_i^\intercal w - y_i)^2$ -->

<!-- With a regularisation term, this becomes: -->

<!-- $\min_w \sum_{i = 1}^{n} (x_i^\intercal w - y_i)^2 + \lambda \Omega(w)$ -->

<!-- $\Omega(w)$ is a penality on the complexity of the model. Two common choices for $\Omega(w)$ are: -->

<!-- 1. $\Omega(w) = ||w||_2^2$: this is ridge regression. -->
<!-- 2. $\Omega(w) = ||w||_1$: this is LASSO. -->

<!-- There are two types of flexibility within this framework: -->

<!-- - Choice of norm, a structural decision. -->
<!-- - Choice of $\lambda$, a parametric decision.	 -->

<!-- Both types of regression shrink all the elements of the unconstrained $w$ vector towards zero, some more than others in a special way. LASSO shrinks the coefficients so that some are equal to zero. This feature is nice because it helps us interpret the model by getting rid of the effects of many of the variables.	 -->

<!-- For LASSO, we'll use the `glmnet` package. This package illustrates some of the less desirable aspects of the R universe, since it doesn't work very elegantly with the `tidyverse` (it uses matrix representations of the data rather than data frame representations). -->

<!-- Let's check out the function `glmnet()`. We can see the documentation from the command line using `?glmnet`.	 -->
<!-- ```{r} -->
<!-- library(glmnet) -->
<!-- ?glmnet -->
<!-- ``` -->

<!-- Notice that `glmnet()` doesn't communicate with the data via formulas. Instead, it wants a matrix of predictor variables and a vector of values for the variable we're trying to predict, including all the categorical variables that R automatically expanded into indicator variables. Fortunately, R has a `model.matrix()` function which takes a data frame and gets it into the right form for `glmnet()` and other functions with this type of input.	 -->

<!-- First, we need to convert the data to a matrix for X and a vector for Y. We'll use the big_formula that we made previously for our linear regression, but with the dependent variable removed. -->

<!-- ```{r} -->
<!-- model_formula <- as.formula(gsub("price", "", paste(big_formula))) -->
<!-- x <- model.matrix(model_formula, data = listingsBig) -->
<!-- y <- as.vector(listingsBig$price) -->
<!-- ``` -->

<!-- Next, split into training/testing sets -->

<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- spl <- sample.split(y, SplitRatio = 0.7) -->
<!-- xTrain <- x[spl, ] -->
<!-- xTest <- x[!spl, ] -->
<!-- yTrain <- y[spl] -->
<!-- yTest <- y[!spl] -->
<!-- ``` -->

<!-- Finally, let's fit a our first LASSO model. There's a way to specify lambda manually, but let's just accept the default for now and see what we get. -->

<!-- ```{r} -->
<!-- lasso_price <- glmnet(xTrain, yTrain) -->
<!-- ``` -->

<!-- This time the `summary()` function isn't quite as useful: -->

<!-- ```{r} -->
<!-- summary(lasso_price) -->
<!-- ``` -->

<!-- It does give us some info, though. Notice that "lambda" is a vector of length 86. The `glmnet()` function has defined 86 different values of lambda and found the corresponding optimal beta vector for each one! We have 86 different models here. -->

<!-- Let's look at some of the coefficients for the different models. We'll start with one where lambda is really high: -->
<!-- ```{r} -->
<!-- lasso_price$lambda[1] -->
<!-- nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?	 -->
<!-- ``` -->

<!-- Here the penalty on the size of the coefficients is so high that R sets them all to zero. Moving to some smaller lambdas: -->

<!-- ```{r} -->
<!-- lasso_price$lambda[10]	 -->
<!-- lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]	 -->

<!-- lasso_price$lambda[20]	 -->
<!-- lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20] -->
<!-- ``` -->

<!-- And, to see the whole path of lambdas: -->

<!-- ```{r} -->
<!-- plot(lasso_price, xvar = "lambda") -->
<!-- ``` -->

<!-- Here, each line is one variable. The plot is quite messy with so many variables, but it gives us the idea. As lambda shrinks, the model adds more and more nonzero coefficients.	 -->


<!-- ## Cross-Validation -->

<!-- How do we choose which of the 86 models to use? Or in other words, how do we "tune" the $\lambda$ parameter? We'll use a similar idea to the training-test set split called cross-validation.	 -->

<!-- The idea behind cross-validation is this: what if we trained our family of models (in this case 86) on only some of the training data and left out some other data points? Then we could use those other data points to figure out which of the lambdas works best out-of-sample. So we'd have a training set for training all the models, a validation set for choosing the best one, and a test set to evaluate performance once we've settled on a model.	 -->

<!-- There's just one other trick: since taking more samples reduces noise, could we somehow take more validation set samples? Here's where *cross*-validation comes in. -->

<!-- We divide the training data into groups called *folds*, and for each fold repeat the train-validate procedure on the remaining training data and use the current fold as a validation set. We then average the performance of each model on each fold and pick the best one.	 -->

<!-- This is a very common *resampling* method that applies in lots and lots of settings. Lucky for us the glmnet package has a very handy function called `cv.glmnet()` which does the entire process automatically. Let's look at the function arguments using `?cv.glmnet`.	 -->

<!-- The relevant arguments for us right now are	 -->
<!-- * x, the matrix of predictors	 -->
<!-- * y, the response variable	 -->
<!-- * nfolds, the number of ways to split the training set (defaults to 10) -->
<!-- * type.measure, the metric of prediction quality. It defaults to mean squared error, the square of RMSE, for linear regression	 -->

<!-- Let's do the cross-validation: -->

<!-- ```{r} -->
<!-- lasso_price_cv <- cv.glmnet(xTrain, yTrain) -->
<!-- summary(lasso_price_cv) # What does the model object look like?	 -->
<!-- ``` -->

<!-- Notice the "lambda.min". This is the best lambda as determined by the cross validation. "lambda.1se" is the largest lambda such that the "error is within 1 standard error of the minimum."	 -->

<!-- There's another automatic plotting function for `cv.glmnet()` which shows the error for each model:	 -->

<!-- ```{r} -->
<!-- plot(lasso_price_cv) -->
<!-- ``` -->

<!-- The first vertical dotted line shows `lambda.min`, and the second is `lambda.1se`. The figure illustrates that we cross-validate to find the "sweet spot" where there's not too much bias (high lambda) and not too much noise (low lambda). The left-hand side of this graph is flatter than we'd sometimes see, meaning that the unpenalized model may not be too bad. However, increasing lambda increases interpretability at close to no loss in prediction accuracy!	 -->

<!-- Let's again compare training and test error. Because we are using glmnet we need to use the specialized `predict.cv.glmnet()` function: -->

<!-- ```{r} -->
<!-- ?predict.cv.glmnet -->

<!-- #pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain, s = "lambda.min") -->
<!-- pred_train <- predict(lasso_price_cv, newx = xTrain, s = "lambda.min") -->
<!-- R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) / -->
<!--   sum((mean(yTrain) - yTrain) ^ 2) -->
<!-- #pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest, s = "lambda.min") -->
<!-- pred_test <- predict(lasso_price_cv, newx = xTest, s = "lambda.min") -->
<!-- OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) / -->
<!--   sum((mean(yTrain) - yTest) ^ 2) -->
<!-- ``` -->

<!-- Let's add these as a row to our results data.frame -->

<!-- ```{r} -->
<!-- results <- results %>% -->
<!--   rbind(list(model = "lasso", R2 = R2_lasso, OSR2 = OSR2_lasso)) -->
<!-- results -->
<!-- ``` -->

<!-- The overfitting problem has gotten better, but hasn't yet gone away completely. I added a bunch variables for dramatic effect that we could probably screen out before running the LASSO if we really wanted a good model. -->

<!-- One more note on cross-validation: the `glmnet` package has built-in functionality for cross-validation. In situations where that's not the case, `modelr::crossv_kfold()` will prepare data for cross-validation in a nice way. -->

## Classification
So far we've looked at models which predict a continuous response variable. There are many related models which predict categorical outcomes, such as whether an email is spam or not, or which digit a handwritten number is. We'll take a brief look at two of these: logistic regression and classification trees.

### Logistic Regression
Logistic regression is part of the class of generalized linear models (GLMs), which build directly on top of linear regression. These models take the linear fit and map it through a non-linear function. Where regression has the form $$y=\beta_0+\beta_1 x_2+\beta_2 x_2$$, whereas a GLM model has the form $$y=f(\beta_0+\beta_1 x_2+\beta_2 x_2)$$ For logistic regression, the function $f()$ is given by $f(x) = 1/(1+\exp(-x))$ (the *logistic function*), which looks like this:	

```{r}
xs <- seq(-10, 10, 0.25)
ys <- exp(xs) / (1 + exp(xs))
plot(xs, ys)
```

Since the function stays between zero and one, it can be interpreted as a mapping from predictor values to a probability of being in one of two classes.	

Let's apply this model to the `listings` data. Let's try to predict which listings have elevators in the building by price. To make sure we're asking a sensible question, we'll only consider apartments priced at $500 or less.

```{r}
source("expand_amenities.R")
listingsBig <- expand_amenities(listings)

listingsGLM <- listingsBig %>%
  filter(property_type == "Apartment", price <= 500)
set.seed(123)
spl <- sample.split(listingsGLM$amenity_Elevator_in_Building, SplitRatio = 0.7)
listingsGLMTrain <- subset(listingsGLM, spl == TRUE)
listingsGLMTest <- subset(listingsGLM, spl == FALSE)
```

One nice thing about using `sample.split` for classification is that it preserves the ratio of class labels in the training and testing sets.

Instead of the `lm()` function, we'll now use `glm()`, but the syntax is almost exactly the same:	

```{r}
l.glm <- glm(amenity_Elevator_in_Building ~ price,
             family = "binomial", data = listingsGLMTrain)
summary(l.glm)
```
Again, we can add predictions to the data frame and plot these along with the actuals, although the result doesn't look nearly as clean:	

```{r}
listingsGLMTest %>%
  mutate(pred = predict(l.glm, newdata = listingsGLMTest, type = "response")) %>%
  ggplot(aes(x = price)) + 
  geom_line(aes(y = pred)) + 
  geom_point(aes(y = amenity_Elevator_in_Building + 0))
```

One way to get a more informative plot is by using the `logi.hist.plot()` function in the `popbio` package.	

In the meantime, we can explore out-of-sample performance. Ultimately, we want to predict whether or not a listing has an elevator. However, logistic regression gives us something a bit different: a probability that each listing has an elevator. This gives us flexibility in the way we predict. The most natural thing would be to predict that any listing with predicted probability above 0.5 *has* an elevator, and any listing with predicted probability below 0.5 *does not have* an elevator. But what if I use a wheelchair and I want to be really confident that there's going to be an elevator? I may want to use a cutoff value of 0.9 rather than 0.5. In fact, we could choose any cutoff value and have a corresponding prediction model.	

There's a really nice metric that measures the quality of all cutoffs simultaneously: *AUC*, for "Area Under the receiver operating characteristic Curve." That's a mouthful, but the idea is simpler: For every cutoff, we'll plot the *false positive rate* against the *true positive rate* and then take the area under this curve. (A *positive* in our case is a listing that has an elevator. So a *true positive* is a listing that we predict has an elevator and really does have an elevator, while a *false positive* is a listing that we predict has an elevator and does *not* actually have an elevator.)	

As the cutoff shrinks down from 1 to 0, the rate of total positives will increase. If the rate of true positives increases faster than the rate of false positives, this is one indication that the model is good. This is what AUC measures.	

The `ROCR` package is one implementation that allows us to plot ROC curves and calculate AUC. Here's an example:	
```{r}
pred_test <- predict(l.glm, newdata = listingsGLMTest, type = "response")	
pred_obj <- prediction(pred_test, listingsGLMTest$amenity_Elevator_in_Building)
# Creating a prediction object for ROCR	
perf <- performance(pred_obj, 'tpr', 'fpr')	
plot(perf, colorize = T)  # ROC curve
performance(pred_obj, 'auc')@y.values  # AUC - a scalar measure of performance	
```
As you can see, the `performance()` function in the `ROCR` package is versatile and allows you to calculate and plot a bunch of different performance metrics. In our case, this model gives an AUC of 0.7. The worst possible is 0.5 - random guessing. We're definitely better than random here, and could likely improve by adding more predictors.	

Evaluation and iteration are also important when choosing classification models. The same technique of splitting the data into a training, validation, and testing sets should used when performing a model selection task.

We've covered basic logistic regression, but just as with linear regression there are many, many extensions. For example, we could do regularized logistic regression if we wanted to use many predictors, using the `glmnet` package.	

## Classification Trees 
We will briefly explore classification trees (often referred to as CART, for Classification And Regression Trees), and then in the second half of the session we'll take a deeper dive.	

A (binary) classification tree makes predictions by grouping similar observations and then assigning a probability to each group using the proportion of observations within that group that belong to the positive class. Groups can be thought of as nodes on a tree, and tree branches correspond to logical criteria on the predictor variables. There's a lot of neat math that goes into building the trees, but we won't get into that today. For now let's get familiarized by looking at a simple example. We will use the `rpart` library.	

The model construction step follows the same established pattern. We use the modelling function `rpart()`, which takes a formula
and a data frame (and optional parameters) as arguments.	
```{r}
l.rpart <- rpart(amenity_Elevator_in_Building ~ price + 
                   neighbourhood_cleansed,
                 data = listingsGLMTrain)	
summary(l.rpart)	
```
This is another case when the `summary()` function is less helpful. We can plot the resulting tree using the `rpart.plot` package:	
```{r}
prp(l.rpart)
```
To evaluate the prediction accuracy of our classification tree, we count up the number of times each of the following occurs:
 * Y = 1, prediction = 1 (True Positive)
 * Y = 0, prediction = 1 (False Positive)
 * Y = 1, prediction = 0 (False Negative)
 * Y = 0, prediction = 0 (True Negative)
A table that holds these values is called a "confusion matrix". Then, accuracy = ( # True Positives +  # True Negatives) / (Total # of observations) 

Let's calculate the training and testing accuracy for our model
```{r}
pred_train <- predict(l.rpart)
confusionMatrixTrain <- table(listingsGLMTrain$amenity_Elevator_in_Building,
                              ifelse(pred_train > 0.5, "pred = 1", "pred = 0"))
accTrain <- sum(diag(confusionMatrixTrain)) / nrow(listingsGLMTrain)
pred_test <- predict(l.rpart, newdata = listingsGLMTest)
confusionMatrixTest <- table(listingsGLMTest$amenity_Elevator_in_Building,
                             ifelse(pred_test > 0.5, "pred = 1", "pred = 0"))
accTest <- sum(diag(confusionMatrixTest)) / nrow(listingsGLMTest)
```

What is the baseline out-of-sample accuracy? This is just the frequency of the most common class in the training set.
```{r}
max(table(listingsGLMTest$amenity_Elevator_in_Building))/nrow(listingsGLMTest)
```

### Tuning the CART model
If we want to construct high accuracy decision tree models, then we need to tune the parameters.  CART has many parameters that specify how the decision tree is constructed, but one of the most important is cp, the complexity parameter. 
cp is a non-negative parameter which typically takes values like 0.1, 0.1, 0.01, 0.001, etc. (default = 0.01). It is the minimum complexity threshold that the CART algorithm uses to decide whether or not to make a split.  So:
*If cp is low => low splitting threshold => big tree
*If cp is high => high splitting threshold => small tree
Similar to lambda in the LASSO model, cp controls the "complexity" of the model, so it important to tune it to avoid over-fitting or under-fitting. You can think of it like this:
*If the tree is too big => too many splits => we have an over-fitting problem.
*If the tree is too small => too few splits => we have an under-fitting problem.
We want to find a tree in the middle, that is "just right". The rpart package makes it easy to perform tune cp via cross-validation. Basically, we start out with a big tree, then "prune" it down to get the right sized tree. Let's begin by constructing a tree with a small cp parameter, which will vary depending upon the problem.  Here, let's do 0.001.

```{r}
treeBig <- rpart(amenity_Elevator_in_Building ~ price + neighbourhood_cleansed,
                 data = listingsGLMTrain,
                 cp = 0.001)
prp(treeBig)
```

We can use the `printcp()` command to see the cross-validated error for different values of cp, where:
* "nsplit"    = number of splits in tree
* "rel error" = scaled training error
* "xerror"    = scaled cross-validation error
* "xstd"      = standard deviation of xerror
```{r}
printcp(treeBig)
```

Cool, so rpart automatically computes all of the cross-validated errors for trees with cp = 0.001 and up! We can also see these results visually using the `plotcp` command.  In this plot:
*size of tree = (number of splits in tree) + 1
*the dotted line occurs at 1 std. dev. above the minimum xerror

```{r}
plotcp(treeBig)
```

A rule of thumb is to select the cp value which first goes below the dotted line, and then prune the tree using this value.
```{r}
treeFinal <- prune(treeBig, cp = 0.011)
prp(treeFinal)
```

In this case, because the best cp value = 0.011 is very close to thedefault cp value of 0.01, this tree is identical to the initial tree that we constructed.  This occurs because the best tree in this case is relatively simple. Note that we could have also computed the confusion matrices, training/testing accuracy, and baseline accuracy for our logistic regression model. We can also try looking at a more detailed graph of the tree 
```{r}
rpart.plot(treeFinal)
```

Let's save the plot as a pdf
```{r}
pdf("finalTree.pdf", width = 5, height = 2)
rpart.plot(treeFinal)
dev.off()
```

#### Exercise 4 
1. Add more variables to Logistic Regression.

Try to beat the out-of-sample performance for logistic regression of elevators on price by adding new variables. Compute the out-of-sample AUC of the final model, and plot the ROC curve.

```{r}
l.glm_2 <- glm(amenity_Elevator_in_Building ~
                 price + neighbourhood_cleansed,
               family = "binomial", data = listingsGLMTrain)
summary(l.glm_2)

pred_test <- predict(l.glm_2, newdata = listingsGLMTest, type = "response")
pred_obj <- prediction(pred_test, listingsGLMTest$amenity_Elevator_in_Building)
perf <- performance(pred_obj, 'tpr', 'fpr')
performance(pred_obj, 'auc')@y.values
plot(perf, colorize = TRUE)
confusionMatrixTest_2 <- table(listingsGLMTest$amenity_Elevator_in_Building,
                               ifelse(pred_test > 0.5, "pred = 1", "pred = 0"))
accTest_2 <- sum(diag(confusionMatrixTest_2)) / nrow(listingsGLMTest)
confusionMatrixTest # CART model
accTest
confusionMatrixTest_2 # Logistic Regression Model
accTest_2
```


2. Tuning a CART model

Let's try building a more complicated CART model and tuning the parameters. Using the below formula, build a CART model to predict `neighbourhood_cleansed` based on price and all of the amenities, tuning the cp parameter.
```{r}
amenities_string <- listingsGLMTrain %>%
  select(starts_with("amenity")) %>%
  names() %>%
  paste(collapse = " + ")
tree_formula <- as.formula(paste("neighbourhood_cleansed ~ price", 
                                 amenities_string, sep = " +  "))
```
Plot the final tree with the option "varlen = 0"
```{r}
treeBig2 <- rpart(tree_formula,
                  data = listingsGLMTrain,
                  cp = 0.001)
plotcp(treeBig2)
treeFinal2 <- prune(treeBig2, 0.0024)
prp(treeFinal2, varlen =  0)
```


## Random Forests
We will briefly take a look at random forests, using the `randomForest` library. A random forest is a collection of slightly randomized decision trees (hence the name "forest"), and can be used for classification or prediction. They often have excellent predictive performance, but can be expensive to train. 

Let's start by training a random forest model for a classification task. We will perform the same task as we did using the simple CART model above, and compare the performance of Random Forest to CART
```{r}
listingsGLMTrain$amenity_Elevator_in_Building=as.factor(listingsGLMTrain$amenity_Elevator_in_Building)
listingsGLMTrain$neighbourhood_cleansed=as.factor(listingsGLMTrain$neighbourhood_cleansed)
rf <- randomForest(amenity_Elevator_in_Building ~ price+neighbourhood_cleansed,
                   data = listingsGLMTrain, ntree = 50)

pred_train <- predict(rf)
confusionMatrixTrain <- table(listingsGLMTrain$amenity_Elevator_in_Building,
                              ifelse(pred_train==TRUE, "pred = 1", "pred = 0"))
accTrain <- sum(diag(confusionMatrixTrain)) / nrow(listingsGLMTrain)

listingsGLMTest$amenity_Elevator_in_Building=as.factor(listingsGLMTest$amenity_Elevator_in_Building)
listingsGLMTest$neighbourhood_cleansed=as.factor(listingsGLMTest$neighbourhood_cleansed)
pred_test <- predict(rf, newdata = listingsGLMTest)
confusionMatrixTest <- table(listingsGLMTest$amenity_Elevator_in_Building,
                             ifelse(pred_test==TRUE, "pred = 1", "pred = 0"))
accTest <- sum(diag(confusionMatrixTest)) / nrow(listingsGLMTest)
```

Compare this to what we got using rpart, which had accTrain=0.8189944 and accTest=0.8031291. So random forest is doing a bit better! It would likely outperform rpart even more if more variables were used and the random forest model were properly tuned.

We will now use Random Forest for a regression task of predicting `price` from `accomodates`
```{r}
set.seed(123)
spl <- sample.split(listings$price, SplitRatio = 0.7)
listingsTrain <- subset(listings, spl == TRUE)
listingsTest <- subset(listings, spl == FALSE)
```

For linear regression, the code was:	
```{r}
lm1 <- lm(price ~ accommodates, data = listingsTrain)
```

Using Random Forest, we can write
```{r}
rf <- randomForest(price ~ accommodates,
                   data = listingsTrain, ntree = 50)
```
We can compare the performance of the random forest model to the linear regression model by plotting the predictions of each model:
```{r}
listingsTrain %>%
  gather_predictions(lm1, rf) %>%
  ggplot(aes(x = accommodates)) +	
  geom_point(aes(y = price)) +	
  geom_line(aes(y = pred, color = model))
```
TODO: Comparing other metrics between the two models...

## Unsupervised Learning
Thus far, our machine learning task has been to predict labels, which were either continuous-valued (for regression) or discrete-valued (for classification).  To do this, we input to the ML algorithms some known (feature, label) examples (the training set), and the ML algorithm outputs a function which enables us to make predictions for some unknown (feature, ?) examples (the testing set).  This problem setup is known as **Supervised Learning**.

Next, we consider **Unsupervised Learning**, where we are not given labelled examples, and we simply run ML algorithms on (feature) data, with the purpose of finding interesting structure and patterns in the data.  Let's run one of the widely-used unsupervised learning algorithms, **k-means clustering**, on the `listings` data frame to explore the Airbnb data set.

First, let's look at help page for the function `k-means()`:
```{r}
help(kmeans)
```

Let's create a new data.frame `listings_numeric` which has the subset of columns that we wish to cluster on.  For the `k-means()` function, all of these columns must be numeric.
```{r}
listings_numeric <- listingsOrig %>%
  select(id, latitude, longitude, accommodates, bathrooms, 
         bedrooms, review_scores_rating, price) %>%
  mutate(price = as.numeric(gsub("\\$|,", "", price))) %>%
  na.omit()
str(listings_numeric)
```
Next, run the **k-means** algorithm on the numeric data.frame, with `k = 5` cluster centroids:
```{r}
set.seed(1234)
kmeans_clust <- kmeans(listings_numeric[,-1:-3],
                       5, iter.max = 1000, nstart = 100)
```

What are the characteristics of these 5 groups?  How many listings are in each cluster?
```{r}
kmeans_clust$centers
table(kmeans_clust$cluster)
```
Finally let's take a look at where the clusters are located; to do this we  will use a package called `leaflet`; additionally to help us get a good color scheme we will use `RColorBrewer`.

To look at color scheme options we can simply type 
```{r}
display.brewer.all()
```

Let's visualize the distribution of clusters from the houses; first we need to add our cluster labels to the data and then we willdefine as color palette that leaflet can use to help us.
```{r}
listings_numeric = listings_numeric %>% 
  mutate(clust_label = as.factor(kmeans_clust$cluster))
```
Now we need to define a color palette that to distinguish the clusters; since we have five cluters we will need five distinct colors
```{r}
pal = colorFactor(palette = "Set1", domain = listings_numeric$clust_label)
```

Now let's plot the houses by cluster
```{r}
leaflet(listings_numeric) %>% 
  addTiles() %>% 
  addCircleMarkers(~longitude, ~latitude, color = ~pal(clust_label))
```

Can you see where the clusters are?  Also, what is the proper number of clusters? We will revisit this in the next session, because it requires some more advanced tidyverse tools.  Stay tuned!

In this module, we have covered examples of machine learning methods for linear regression (ordinary and penalized) and classification (supervised and unsupervised).  This is just the tip of the iceberg.  There are tons more machine learning methods which can be easily implemented in R.  